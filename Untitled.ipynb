{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a3b9cff-79f7-44b0-8e72-64cc77b30522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 scraped successfully.\n",
      "Page 2 scraped successfully.\n",
      "Page 3 scraped successfully.\n",
      "Page 4 scraped successfully.\n",
      "Page 5 scraped successfully.\n",
      "Page 6 scraped successfully.\n",
      "Page 7 scraped successfully.\n",
      "Page 8 scraped successfully.\n",
      "Page 9 scraped successfully.\n",
      "Page 10 scraped successfully.\n",
      "Page 11 scraped successfully.\n",
      "Page 12 scraped successfully.\n",
      "Page 13 scraped successfully.\n",
      "Page 14 scraped successfully.\n",
      "Page 15 scraped successfully.\n",
      "Page 16 scraped successfully.\n",
      "Page 17 scraped successfully.\n",
      "Page 18 scraped successfully.\n",
      "Page 19 scraped successfully.\n",
      "Page 20 scraped successfully.\n",
      "Page 21 scraped successfully.\n",
      "Page 22 scraped successfully.\n",
      "Page 23 scraped successfully.\n",
      "Page 24 scraped successfully.\n",
      "Page 25 scraped successfully.\n",
      "Page 26 scraped successfully.\n",
      "Page 27 scraped successfully.\n",
      "Page 28 scraped successfully.\n",
      "Page 29 scraped successfully.\n",
      "Page 30 scraped successfully.\n",
      "Page 31 scraped successfully.\n",
      "Page 32 scraped successfully.\n",
      "Page 33 scraped successfully.\n",
      "Page 34 scraped successfully.\n",
      "Page 35 scraped successfully.\n",
      "Page 36 scraped successfully.\n",
      "Page 37 scraped successfully.\n",
      "Page 38 scraped successfully.\n",
      "Page 39 scraped successfully.\n",
      "Page 40 scraped successfully.\n",
      "Page 41 scraped successfully.\n",
      "Page 42 scraped successfully.\n",
      "Page 43 scraped successfully.\n",
      "Page 44 scraped successfully.\n",
      "Page 45 scraped successfully.\n",
      "Page 46 scraped successfully.\n",
      "Page 47 scraped successfully.\n",
      "Page 48 scraped successfully.\n",
      "Page 49 scraped successfully.\n",
      "Page 50 scraped successfully.\n",
      "Scraping complete. Filtered data saved to 'businesswire_news_filtered.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "base_url = \"https://www.businesswire.com/portal/site/home/news/industry/?vnsId=31249&page=\"\n",
    "keywords_to_search = ['Energy', 'Electric Vehicle']\n",
    "headlines = []\n",
    "dates = []\n",
    "hrefs = []\n",
    "\n",
    "for page_num in range(1, 51):  \n",
    "    url = base_url + str(page_num)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page {page_num}\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    news_items = soup.find_all(\"li\")\n",
    "\n",
    "    for item in news_items:\n",
    "        headline_tag = item.find(\"span\", itemprop=\"headline\")\n",
    "        href_tag = item.find(\"a\", class_=\"bwTitleLink\")\n",
    "        date_tag = item.find(\"time\", itemprop=\"dateModified\")\n",
    "        \n",
    "        if headline_tag and href_tag and date_tag:\n",
    "            headline = headline_tag.get_text(strip=True)\n",
    "            href = href_tag[\"href\"]\n",
    "            full_href = \"https://www.businesswire.com\" + href\n",
    "            date = date_tag.get_text(strip=True)\n",
    "            \n",
    "            if any(keyword.lower() in headline.lower() for keyword in keywords_to_search):\n",
    "                headlines.append(headline)\n",
    "                dates.append(date)\n",
    "                hrefs.append(full_href)\n",
    "    \n",
    "    print(f\"Page {page_num} scraped successfully.\")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Headline\": headlines,\n",
    "    \"Date\": dates,\n",
    "    \"Link\": hrefs\n",
    "})\n",
    "\n",
    "df.to_csv(\"businesswire_news_filtered.csv\", index=False)\n",
    "\n",
    "print(\"Scraping complete. Filtered data saved to 'businesswire_news_filtered.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cda562-2fa8-4b42-bb98-2e3b920fe9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
